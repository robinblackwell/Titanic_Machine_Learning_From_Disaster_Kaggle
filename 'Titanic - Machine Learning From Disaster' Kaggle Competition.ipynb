{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Titanic - Machine Learning From Disaster' Kaggle Competition\n",
    "\n",
    "Submission notebook used to obtain a final accuracy of 0.79665, placing in top 5% on leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statistics\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1). Data cleaning and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "data_train = pd.read_csv(\"./Data/train.csv\")\n",
    "data_test = pd.read_csv(\"./Data/test.csv\")\n",
    "data_train.set_index('PassengerId', inplace=True)\n",
    "data_test.set_index('PassengerId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['TrainTest'] = 1\n",
    "data_test['TrainTest'] = 0\n",
    "data_test['Survived'] = np.NaN\n",
    "data_all = pd.concat([data_train,data_test], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age           263\n",
       "Cabin        1014\n",
       "Embarked        2\n",
       "Fare            1\n",
       "Name            0\n",
       "Parch           0\n",
       "Pclass          0\n",
       "Sex             0\n",
       "SibSp           0\n",
       "Survived      418\n",
       "Ticket          0\n",
       "TrainTest       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how complete is the data?\n",
    "data_all.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2). Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['Title'] = data_all['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "data_all['CabinCount'] = data_all['Cabin'].apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n",
    "data_all['CabinDeck'] = data_all['Cabin'].apply(lambda x: str(x)[0] if not pd.isna(x) else 'None')\n",
    "data_all['TicketLetters'] = data_all['Ticket'].apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) > 0 else 'None')\n",
    "data_all['IsTicketNumber'] = data_all['Ticket'].apply(lambda x: 1 if x.isnumeric() else 0)\n",
    "\n",
    "data_all.drop(columns = ['Ticket', 'Name', 'Cabin'], inplace=True)\n",
    "\n",
    "data_all.dropna(subset = ['Embarked'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['Age'] = data_all['Age'].fillna(data_all[data_all['TrainTest'] == 1]['Age'].median())\n",
    "data_all['Fare'] = data_all['Fare'].fillna(data_all[data_all['TrainTest'] == 1]['Fare'].median())\n",
    "\n",
    "#log norm of fare\n",
    "data_all['FareNorm'] = np.log(data_all['Fare']+1)\n",
    "data_all.drop(columns = ['Fare'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                 0\n",
       "Embarked            0\n",
       "Parch               0\n",
       "Pclass              0\n",
       "Sex                 0\n",
       "SibSp               0\n",
       "Survived          418\n",
       "TrainTest           0\n",
       "Title               0\n",
       "CabinCount          0\n",
       "CabinDeck           0\n",
       "TicketLetters       0\n",
       "IsTicketNumber      0\n",
       "FareNorm            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double check imputation was successful\n",
    "data_all.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age               float64\n",
       "Embarked           object\n",
       "Parch               int64\n",
       "Pclass              int64\n",
       "Sex                object\n",
       "SibSp               int64\n",
       "Survived          float64\n",
       "TrainTest           int64\n",
       "Title              object\n",
       "CabinCount          int64\n",
       "CabinDeck          object\n",
       "TicketLetters      object\n",
       "IsTicketNumber      int64\n",
       "FareNorm          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we then need to handle the categorical data using OneHotEncoder\n",
    "\n",
    "data_all['Pclass'] = data_all['Pclass'].astype(str)\n",
    "\n",
    "all_dummies = pd.get_dummies(data_all[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'FareNorm', 'Embarked', 'CabinCount', 'CabinDeck', 'IsTicketNumber', 'Title', 'TicketLetters', 'TrainTest']])\n",
    "\n",
    "X_train = all_dummies[all_dummies['TrainTest'] == 1].drop(['TrainTest'], axis=1)\n",
    "X_test = all_dummies[all_dummies['TrainTest'] == 0].drop(['TrainTest'], axis=1)\n",
    "\n",
    "y_train = data_all[data_all['TrainTest'] == 1]['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                     float64\n",
       "SibSp                     int64\n",
       "Parch                     int64\n",
       "FareNorm                float64\n",
       "CabinCount                int64\n",
       "                         ...   \n",
       "TicketLetters_stono2      uint8\n",
       "TicketLetters_stonoq      uint8\n",
       "TicketLetters_swpp        uint8\n",
       "TicketLetters_wc          uint8\n",
       "TicketLetters_wep         uint8\n",
       "Length: 77, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that encoding has been successful\n",
    "all_dummies.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3). Data preprocessing for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data \n",
    "scale = StandardScaler()\n",
    "all_dummies_scaled = all_dummies.copy()\n",
    "all_dummies_scaled[['Age','SibSp','Parch','FareNorm']] = scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','FareNorm']])\n",
    "\n",
    "X_train_scaled = all_dummies_scaled[all_dummies_scaled['TrainTest'] == 1].drop(['TrainTest'], axis=1)\n",
    "X_test_scaled = all_dummies_scaled[all_dummies_scaled['TrainTest'] == 0].drop(['TrainTest'], axis=1)\n",
    "\n",
    "y_train = data_all[data_all['TrainTest'] == 1]['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4). Build basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.803 (0.037)\n"
     ]
    }
   ],
   "source": [
    "#Create a Gaussian Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "n_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)\n",
    "\n",
    "#report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (statistics.mean(n_scores), statistics.stdev(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5). Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "param_grid_random =  {'n_estimators': [100, 500, 1000], \n",
    "                                  'bootstrap': [True, False],\n",
    "                                  'max_depth': [3, 5, 10, 20, 50, 75, 100, None],\n",
    "                                  'max_features': ['auto', 'sqrt'],\n",
    "                                  'min_samples_leaf': [1, 2, 4, 10],\n",
    "                                  'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "clf_randomsearch = RandomizedSearchCV(clf, param_distributions=param_grid_random, n_iter=100, cv=5, n_jobs=-1)\n",
    "best_clf_randomsearch = clf_randomsearch.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8346456692913385\n",
      "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 50, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "print(best_clf_randomsearch.best_score_)\n",
    "print(best_clf_randomsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [80, 90, 100, 110, 120],\n",
    "               'criterion': ['gini','entropy'],\n",
    "                                  'bootstrap': [True],\n",
    "                                  'max_depth': [90, 100, 110],\n",
    "                                  'max_features': ['auto', 'sqrt', 10],\n",
    "                                  'min_samples_leaf': [1, 2, 3],\n",
    "                                  'min_samples_split': [8, 10, 12]}\n",
    "\n",
    "clf_gridsearch = GridSearchCV(clf, param_grid = param_grid, cv = 5, n_jobs = -1)\n",
    "best_clf_gridsearch = clf_gridsearch.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8402699662542182\n",
      "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 90, 'max_features': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 80}\n"
     ]
    }
   ],
   "source": [
    "print(best_clf_gridsearch.best_score_)\n",
    "print(best_clf_gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##identify most important features\n",
    "#best_clf = best_clf_gridsearch.best_estimator_.fit(X_train_scaled, y_train)\n",
    "#feat_importances = pd.Series(best_clf.feature_importances_, index=X_train_scaled.columns)\n",
    "#feat_importances.nlargest(20).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##select features with more than 0.05 importance\n",
    "#top_features = list(feat_importances.sort_values(ascending=False)[:18].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.828 (0.026)\n"
     ]
    }
   ],
   "source": [
    "clf_optimised = best_clf_gridsearch.best_estimator_\n",
    "\n",
    "n_scores_optimised = cross_val_score(clf_optimised, X_train_scaled, y_train, cv=5)\n",
    "\n",
    "#report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (statistics.mean(n_scores_optimised), statistics.stdev(n_scores_optimised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6). Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = best_clf_gridsearch.best_estimator_.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_upload = pd.DataFrame({'PassengerId': X_test_scaled.index, 'Survived': list(y_pred)})\n",
    "to_upload = to_upload.astype({\"PassengerId\": int, \"Survived\": int})\n",
    "to_upload.to_csv('RB_submission_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
